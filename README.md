# ğŸš€ RAGx - Advanced RAG System with Corrective Methods

Retrieval-Augmented Generation (RAG) system with a bunch of corrective methods including Cross-Encoder reranking, Chain-of-Retrieval (CoRAG), and Chain-of-Verification (CoVe).

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Configuration](#ï¸-configuration)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Documentation](#-documentation)
- [Performance](#-performance)
- [Contributing](#-contributing)

---

## âœ¨ Features

### Core RAG Pipeline
- **ğŸ” Semantic Retrieval** - Multi-lingual embeddings (GTE, E5, BGE)
- **âš¡ Vector Store** - Qdrant with HNSW indexing for fast similarity search
- **ğŸ¯ Cross-Encoder Reranking** - Improves precision by re-scoring top-K results
- **ğŸ¤– LLM Integration** - Qwen2.5, LLaMA, Mistral (inference-only, no training)

### Advanced Methods
- **ğŸ”— Chain-of-Retrieval (CoRAG)** - Multi-step retrieval for complex queries
- **âœ… Self-Verification (CoVe)** - Fact-checking and hallucination reduction
- **ğŸ“ Citation Enforcement** - Inline source citations `[N]` for every claim
- **ğŸ§© Semantic Chunking** - Context-aware text splitting with LlamaIndex

### Production Features
- **ğŸ“Š Progress Tracking** - Resume ingestion from where you left off
- **ğŸ”„ Incremental Indexing** - Skip already processed files
- **âš™ï¸ Configurable Pipeline** - YAML + .env for easy configuration
- **ğŸ“ˆ Performance Monitoring** - Built-in metrics and logging

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Retrieval (Bi-Encoder)                             â”‚
â”‚  - Embed query: "query: <text>"                             â”‚
â”‚  - Search Qdrant: Top-K=80                                  â”‚
â”‚  - Output: 80 candidate chunks                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Reranking (Cross-Encoder)                          â”‚
â”‚  - Score each (query, chunk) pair                           â”‚
â”‚  - Sort by relevance                                        â”‚
â”‚  - Output: Top-N=6 best chunks                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Prompt Engineering                                 â”‚
â”‚  - System: "Answer using sources, cite as [N]"              â”‚
â”‚  - Context: Numbered chunks [1]..[6]                        â”‚
â”‚  - Query: User question                                     â”‚
â”‚  - Instructions: Length, format, fallback                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: LLM Generation                                      â”‚
â”‚  - Model: Qwen2.5-7B-Instruct (4-bit quantized)             â”‚
â”‚  - Temperature: 0.2 (factual)                               â”‚
â”‚  - Output: Answer with citations                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Verification (Optional - CoVe)                     â”‚
â”‚  - Extract claims from answer                               â”‚
â”‚  - Generate verification questions                          â”‚
â”‚  - Re-retrieve evidence for each claim                      â”‚
â”‚  - Correct inconsistencies                                  â”‚
â”‚  - Output: Verified answer                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Final Answer to User                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.12+**
- **Docker** (for Qdrant)
- **20GB+ RAM** (32GB recommended)
- **CUDA GPU** (optional, for faster processing)

### 1. Clone & Install

```bash
git clone https://github.com/floressek/ragx.git
cd ragx

# Install dependencies
make install

# Or manually:
pip install -e .
```

### 2. Start Qdrant

```bash
make setup-qdrant
# Or manually:
docker-compose up -d qdrant
```

### 3. Configure

```bash
# Copy example config
cp .env.example .env

# Edit .env with your settings
# Key variables:
# - EMBEDDING_MODEL
# - QDRANT_COLLECTION
# - CHUNK_SIZE, CHUNK_OVERLAP
```

### 4. Ingest Data

```bash
# Download Polish Wikipedia (small chunk for testing)
make download-wiki

# Extract articles
make extract-wiki

# Index into Qdrant (1k articles for testing)
make ingest-test

# Or full ingestion (200k articles):
# make ingest-full
```

### 5. Search!

```bash
# Try a search
make search QUERY="sztuczna inteligencja"

# Check status
make status
```

---

## ğŸ’» Installation

### Option 1: Using `uv` (Recommended - Fast!)

```bash
# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install project
uv pip install --system -e .
```

### Option 2: Using `pip`

```bash
pip install --upgrade pip
pip install -e .
```

### Option 3: Using `make`

```bash
make install
```

### Dependencies

Core libraries:
- `sentence-transformers` - Embeddings & reranking
- `qdrant-client` - Vector database
- `transformers` - LLM inference
- `llama-index` - Semantic chunking
- `langchain` - Text processing utilities

---

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
# Vector Store
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=ragx_documents_v2

# Embeddings
EMBEDDING_MODEL=Alibaba-NLP/gte-multilingual-base
EMBEDDING_BATCH_SIZE=64
EMBEDDING_USE_PREFIXES=true

# Reranking
RERANKER_MODEL=jinaai/jina-reranker-v2-base-multilingual
RERANKER_BATCH_SIZE=16

# LLM
LLM_MODEL=Qwen/Qwen2.5-7B-Instruct
LLM_LOAD_IN_4BIT=true
LLM_MAX_NEW_TOKENS=300
LLM_TEMPERATURE=0.2

# Retrieval Pipeline
TOP_K_RETRIEVE=80      # Initial retrieval
RERANK_TOP_M=50        # Candidates for reranking
CONTEXT_TOP_N=6        # Final chunks to LLM

# Chunking
CHUNKER_STRATEGY=semantic
CHUNK_SIZE=512
CHUNK_OVERLAP=96
```

### YAML Configuration (Advanced)

See `configs/models.yaml` for detailed model settings:
- HNSW parameters
- Quantization settings
- Chunking strategies
- Deduplication rules

---

## ğŸ“– Usage

### Command-Line Interface

```bash
# Ingestion pipeline
python -m src.ragx.ingestion.pipeline --help

# Available commands:
python -m src.ragx.ingestion.pipeline download --language pl
python -m src.ragx.ingestion.pipeline ingest <source> --max-articles 10000
python -m src.ragx.ingestion.pipeline status
python -m src.ragx.ingestion.pipeline search "query text"
```

### Makefile Commands

```bash
# Setup
make install              # Install dependencies
make setup-qdrant         # Start Qdrant container

# Wikipedia Pipeline
make download-wiki        # Download PL Wikipedia dump
make extract-wiki         # Extract articles to JSON
make ingest-test          # Test ingestion (1k articles)
make ingest-full          # Full ingestion (200k articles)

# Progress Tracking (NEW!)
make ingest-resume        # Resume from last processed file
make ingest-from FILE=wiki_05  # Start from specific file
make status-detailed      # Show file-by-file history

# Search & Status
make search QUERY="..."   # Search for query
make status               # Check system status

# Maintenance
make clean                # Clean cache files
make clean-data           # Clean data files
make clean-all            # Nuclear clean
```

---

## ğŸ“ Project Structure

```
ragx/
â”œâ”€â”€ src/ragx/
â”‚   â”œâ”€â”€ ingestion/              # Data ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ chunkers/
â”‚   â”‚   â”‚   â””â”€â”€ chunker.py      # Semantic & token-based chunking
â”‚   â”‚   â”œâ”€â”€ ingestion_pipeline.py
â”‚   â”‚   â”œâ”€â”€ ingestion_progress.py  # Progress tracking
â”‚   â”‚   â”œâ”€â”€ wiki_extractor.py   # Wikipedia extraction
â”‚   â”‚   â””â”€â”€ pipeline.py         # CLI commands
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/              # Retrieval & reranking
â”‚   â”‚   â”œâ”€â”€ embedder.py         # Bi-encoder embeddings
â”‚   â”‚   â”œâ”€â”€ reranker.py         # Cross-encoder reranking
â”‚   â”‚   â”œâ”€â”€ vector_stores/
â”‚   â”‚   â”‚   â””â”€â”€ qdrant_store.py # Qdrant integration
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/             # LLM & prompting
â”‚   â”‚   â”œâ”€â”€ model.py            # LLM loading
â”‚   â”‚   â”œâ”€â”€ inference.py        # Generation logic
â”‚   â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”‚   â”œâ”€â”€ builder.py      # Prompt templates
â”‚   â”‚   â”‚   â””â”€â”€ heuristics.py   # CoRAG triggers
â”‚   â”‚   â””â”€â”€ providers/          # LLM backends
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # FastAPI server (TODO)
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€â”€ routers/
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ settings.py         # Configuration management
â”‚       â”œâ”€â”€ logging_config.py   # Structured logging
â”‚       â””â”€â”€ model_registry.py   # Model caching
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ models.yaml             # Model configurations
â”‚   â””â”€â”€ app.yaml                # App settings
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Raw Wikipedia dumps
â”‚   â”œâ”€â”€ processed/              # Extracted articles
â”‚   â””â”€â”€ .ingestion_progress.json  # Progress tracking
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest_wiki.py          # Standalone ingestion script
â”‚
â”œâ”€â”€ tests/                      # Unit & integration tests
â”œâ”€â”€ docs/                       # Additional documentation
â”œâ”€â”€ docker-compose.yml          # Qdrant service
â”œâ”€â”€ Dockerfile                  # Application container
â”œâ”€â”€ Makefile                    # Development commands
â”œâ”€â”€ pyproject.toml              # Project dependencies
â””â”€â”€ README.md                   # This file
```

---

## ğŸ“š Documentation

### Technical Specs
- **Architecture Design** - See plan document (TBD)
- **Model Selection** - Embedding vs Reranking tradeoffs
- **Chunking Strategies** - Semantic vs Token-based
- **Performance Tuning** - HNSW, quantization, batching

### API Documentation
- **REST API** - FastAPI endpoints (coming soon)
- **Python SDK** - Programmatic usage examples
- **Configuration Reference** - All .env variables explained

---

## ğŸ“Š Performance

### Benchmarks (Single GPU - RTX 4070)

| Operation | Speed | Notes |
|-----------|-------|-------|
| Embedding (batch=64) | ~1200 docs/s | GTE-multilingual-base |
| Reranking (batch=16) | ~80 pairs/s | Jina-reranker-v2 |
| Chunking (semantic) | ~50 docs/s | LlamaIndex splitter |
| LLM Generation | ~25 tokens/s | Qwen2.5-7B (4-bit) |
| Vector Search | <10ms | Qdrant HNSW (100k docs) |

### Scalability

| Corpus Size | Index Time | Search Time | Memory |
|-------------|------------|-------------|--------|
| 10k docs    | ~5 min     | <10ms | 2GB    |
| 200k docs   | ~3 hours   | <15ms | 8GB    |
| 1M docs     | ~8 hours   | <30ms | 40GB   |

**Optimizations:**
- HNSW on-disk for large indices
- Batched processing for throughput
- Progress tracking for fault tolerance

---

## ğŸ¯ Roadmap

### âœ… Completed (v0.1)
- [x] Basic RAG pipeline (retrieval â†’ LLM)
- [x] Cross-encoder reranking
- [x] Semantic chunking with LlamaIndex
- [x] Qdrant integration with HNSW
- [x] Progress tracking & resume
- [x] Multi-lingual support (PL/EN)

### ğŸš§ In Progress (v0.2)
- [ ] Chain-of-Retrieval (CoRAG) implementation
- [ ] Self-Verification (CoVe) implementation
- [ ] FastAPI REST server
- [ ] Web UI for search
- [ ] Batch evaluation framework

### ğŸ”® Planned (v0.3+)
- [ ] Hybrid search (BM25 + vector)
- [ ] Query expansion & reformulation
- [ ] Multi-hop reasoning
- [ ] Fine-tuning scripts (optional)
- [ ] Deployment guides (Docker, K8s)

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Unit tests only
pytest tests/unit/

# Integration tests (requires Qdrant)
pytest tests/integration/

# With coverage
pytest --cov=src/ragx --cov-report=html
```

---

## ğŸ¤ Contributing

Contributions welcome! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Commit** your changes (`git commit -m 'Add amazing feature'`)
4. **Push** to the branch (`git push origin feature/amazing-feature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Install dev dependencies
make dev

# Pre-commit hooks
pre-commit install

# Code formatting
make fmt

# Linting
make lint
```

### Code Style
- **Formatter:** `black` + `isort`
- **Linter:** `ruff`
- **Type checking:** `mypy`
- **Docstrings:** Google style

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### Papers & Methods
- **RAG:** [Retrieval-Augmented Generation (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)
- **CoRAG:** [Chain-of-Retrieval (Wang et al., 2023)](https://arxiv.org/abs/2401.15884)
- **CoVe:** [Chain-of-Verification (Dhuliawala et al., 2023)](https://arxiv.org/abs/2309.11495)
- **Cross-Encoders:** [Sentence-BERT (Reimers & Gurevych, 2019)](https://arxiv.org/abs/1908.10084)

### Libraries & Tools
- [Sentence-Transformers](https://www.sbert.net/) - Embedding & reranking
- [Qdrant](https://qdrant.tech/) - Vector database
- [LlamaIndex](https://www.llamaindex.ai/) - Semantic chunking
- [Transformers](https://huggingface.co/transformers/) - LLM inference

### Models
- **GTE-multilingual** (Alibaba)
- **Jina-reranker-v2** (Jina AI)
- **Qwen2.5** (Alibaba Cloud)

---

## ğŸ“§ Contact

**Project Maintainer:** Szymon Florek

- **GitHub:** [@floressek](https://github.com/floressek)
- **Email:** your.email@example.com

**Issues & Questions:** [GitHub Issues](https://github.com/floressek/ragx/issues)

---

## ğŸŒŸ Star History

If you find this project useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=floressek/ragx&type=Date)](https://star-history.com/#floressek/ragx&Date)

---

## ğŸ”¥ Quick Examples

### Example 1: Simple Search

```bash
$ make search QUERY="sztuczna inteligencja"

Search results for: 'sztuczna inteligencja'

1. Score: 0.8456
   Doc: Sztuczna inteligencja
   Chunk: 1/12
   Text: Sztuczna inteligencja (SI, ang. artificial intelligence, AI) 
         â€“ dziaÅ‚ informatyki zajmujÄ…cy siÄ™...

2. Score: 0.8123
   Doc: Uczenie maszynowe
   ...
```

### Example 2: Resume Ingestion

```bash
$ make ingest-full
Processing file: wiki_00
[Ctrl+C]

$ make status
Files completed: 5
Current file: wiki_05

$ make ingest-resume
âœ“ Loaded progress
Skipping: wiki_00, wiki_01, ..., wiki_04
Continuing from: wiki_05
```

### Example 3: Python API

```python
from src.ragx.retrieval.embedder import Embedder
from src.ragx.retrieval.vector_stores.qdrant_store import QdrantStore
from src.ragx.retrieval.reranker import Reranker

# Initialize
embedder = Embedder()
store = QdrantStore()
reranker = Reranker()

# Search
query = "What is machine learning?"
query_vec = embedder.embed_query(query)

# Retrieve + Rerank
candidates = store.search(query_vec, top_k=50)
results = reranker.rerank(query, candidates, top_k=5)

# Print
for doc, score in results:
    print(f"Score: {score:.4f} - {doc['text'][:100]}...")
```

---

**Happy RAG-ing! ğŸš€**
