# ğŸš€ RAGx - Advanced RAG System with Query Rewriting & Multihop Retrieval

Retrieval-Augmented Generation (RAG) system with advanced query processing including **Linguistic Analysis**, **Adaptive Query Rewriting**, **Multihop Retrieval**, and **Cross-Encoder Reranking**.

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Configuration](#ï¸-configuration)
- [Usage](#-usage)
- [API Documentation](#-api-documentation)
- [Project Structure](#-project-structure)
- [Performance](#-performance)
- [Contributing](#-contributing)

---

## âœ¨ Features

### Core RAG Pipeline
- **ğŸ” Semantic Retrieval** - Multi-lingual embeddings (GTE, E5, BGE)
- **âš¡ Vector Store** - Qdrant with HNSW indexing for fast similarity search
- **ğŸ¯ Cross-Encoder Reranking** - Improves precision by re-scoring top-K results
- **ğŸ¤– Multi-Provider LLM** - HuggingFace, Ollama, vLLM, API (LM Studio, OpenAI-compatible)

### Advanced Query Processing ğŸ†•
- **ğŸ§  Linguistic Analysis** - spaCy-based POS tagging, dependency parsing, NER
- **ğŸ”„ Adaptive Query Rewriting** - LLM-powered query decomposition and expansion
- **ğŸ¯ Query Type Detection** - Automatic detection of:
    - **Verification** - "Is X the largest...?"
    - **Comparison** - "X vs Y in terms of Z"
    - **Similarity** - "What do X and Y have in common?"
    - **Chaining** - "Who directed the movie starring X?"
    - **Temporal** - "Events between X and Y"
    - **Aggregation** - "How many X..."
    - **Superlative** - "What's the best X under Y?"
- **ğŸ“Š Sub-Query Decomposition** - Intelligent breaking down of complex questions
- **ğŸ”— Multihop Retrieval** - Three-stage reranking (local â†’ fusion â†’ global)

### Advanced Methods
- **âœ… Citation Enforcement** - Inline source citations `[N]` for every claim
- **ğŸ“ Advanced Prompting** - Template system with CoT support and language detection
- **ğŸ§© Semantic Chunking** - Context-aware text splitting with LlamaIndex
- **ğŸ¨ Query-Type-Specific Fusion** - Adaptive weights based on query complexity

### Production Features
- **ğŸ“Š Progress Tracking** - Resume ingestion from where you left off
- **ğŸ”„ Incremental Indexing** - Skip already processed files
- **âš™ï¸ Configurable Pipeline** - YAML + .env for easy configuration
- **ğŸŒ REST API** - FastAPI server with multiple endpoints
- **ğŸ“ˆ Performance Monitoring** - Built-in metrics and logging

---

## ğŸ—ï¸ Architecture

### Enhanced Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    User Query                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Linguistic Analysis (spaCy)                        â”‚
â”‚  - POS tagging, dependency parsing, NER                     â”‚
â”‚  - Syntax depth, clause counting                            â”‚
â”‚  - Entity extraction                                        â”‚
â”‚  Output: LinguisticFeatures                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Adaptive Query Rewriting (LLM)                     â”‚
â”‚  - Query type detection (comparison, similarity, etc.)      â”‚
â”‚  - Decision: decompose / expand / passthrough               â”‚
â”‚  - Multi-hop decomposition into sub-queries                 â”‚
â”‚  Output: is_multihop, sub_queries[], query_type            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                   â”‚
         [Simple]            [Multihop]
              â”‚                   â”‚
              â–¼                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Single Query     â”‚  â”‚ Multiple Sub-Queries     â”‚
   â”‚ Retrieval        â”‚  â”‚ Parallel Retrieval       â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â–¼                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Standard         â”‚  â”‚ Three-Stage Reranking:   â”‚
   â”‚ Reranking        â”‚  â”‚ 1. Local (per subquery)  â”‚
   â”‚ (Cross-Encoder)  â”‚  â”‚ 2. Fusion (by doc_id)    â”‚
   â”‚                  â”‚  â”‚ 3. Global (original Q)   â”‚
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Prompt Engineering                                 â”‚
â”‚  - Template selection (basic/enhanced/multihop)             â”‚
â”‚  - Context formatting with metadata                         â”‚
â”‚  - Language detection & CoT injection                       â”‚
â”‚  - Citation instructions                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: LLM Generation                                      â”‚
â”‚  - Multi-provider support (HF/Ollama/vLLM/API)              â”‚
â”‚  - Temperature control (0.2-0.7)                            â”‚
â”‚  - Chain-of-Thought reasoning                               â”‚
â”‚  Output: Answer with inline citations [N]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Final Answer to User                        â”‚
â”‚  + Source documents with scores                             â”‚
â”‚  + Metadata (timings, query_type, sub_queries)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Type Examples

**Comparison Query:**
```
User: "ziemniaki vs pomidory, co ma wiÄ™cej bÅ‚onnika?"
â””â”€> Detected: comparison
â””â”€> Sub-queries:
    1. "Ile bÅ‚onnika majÄ… ziemniaki?"
    2. "Ile bÅ‚onnika majÄ… pomidory?"
â””â”€> Fusion: MAX strategy
â””â”€> Answer: "Ziemniaki zawierajÄ… okoÅ‚o 2.2g bÅ‚onnika na 100g [1], 
            podczas gdy pomidory okoÅ‚o 1.2g [2]."
```

**Verification Query:**
```
User: "Polska to najwiÄ™kszy kraj europejski?"
â””â”€> Detected: verification
â””â”€> Sub-queries:
    1. "Jaka jest powierzchnia Polski?"
    2. "Jaki jest najwiÄ™kszy kraj w Europie?"
â””â”€> Fusion: Query-type weight = 0.2 (trust local more)
â””â”€> Answer: "Nie, Polska nie jest najwiÄ™kszym krajem w Europie..."
```

---

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.12+**
- **Docker** (for Qdrant)
- **20GB+ RAM** (32GB recommended)
- **CUDA GPU** (optional, for faster processing)

### 1. Clone & Install

```bash
git clone https://github.com/floressek/ragx.git
cd ragx

# Install dependencies
make install

# Or manually:
pip install -e .
```

### 2. Start Qdrant

```bash
make setup-qdrant
# Or manually:
docker-compose up -d qdrant
```

> Dataset used for qdrant:
https://huggingface.co/datasets/Floressek/wiki-1m-qdrant-snapshot

### 3. Configure

```bash
# Copy example config
cp .env.example .env

# Edit .env with your settings
# Key variables:
# - EMBEDDING_MODEL
# - QDRANT_COLLECTION
# - CHUNK_SIZE, CHUNK_OVERLAP
# - REWRITE_ENABLED=true          # Enable query rewriting
# - LLM_PROVIDER=huggingface      # or ollama, vllm, api
```

### 4. Ingest Data

```bash
# Download Polish Wikipedia (small chunk for testing)
make download-wiki

# Extract articles
make extract-wiki

# Index into Qdrant (1k articles for testing)
make ingest-test

# Or full ingestion (200k articles):
# make ingest-full
```

### 5. Start API Server

```bash
# Start FastAPI server
make api

# Or manually:
python -m uvicorn src.ragx.api.main:app --host 0.0.0.0 --port 8000
```

### 6. Try It Out!

```bash
# Simple search
curl -X POST "http://localhost:8000/ask/baseline" \
  -H "Content-Type: application/json" \
  -d '{"query": "sztuczna inteligencja"}'

# Enhanced pipeline with query rewriting
curl -X POST "http://localhost:8000/ask/enhanced" \
  -H "Content-Type: application/json" \
  -d '{"query": "ziemniaki vs pomidory bÅ‚onnik"}'

# Linguistic analysis
curl -X POST "http://localhost:8000/analysis/linguistic" \
  -H "Content-Type: application/json" \
  -d '{"query": "Co Å‚Ä…czy mitologiÄ™ sÅ‚owiaÅ„skÄ… i nordyckÄ…?"}'
```

---

## ğŸ’» Installation

### Option 1: Using `uv` (Recommended - Fast!)

```bash
# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install project
uv pip install --system -e .
```

### Option 2: Using `pip`

```bash
pip install --upgrade pip
pip install -e .
```

### Option 3: Using `make`

```bash
make install
```

### Dependencies

Core libraries:
- `sentence-transformers` - Embeddings & reranking
- `qdrant-client` - Vector database
- `transformers` - LLM inference
- `llama-index` - Semantic chunking
- `spacy` - Linguistic analysis
- `fastapi` - REST API
- `pyyaml` - Configuration

### spaCy Models

```bash
# Polish language model (recommended)
python -m spacy download pl_core_news_md

# English fallback
python -m spacy download en_core_web_sm
```

---

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
# Vector Store
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=ragx_documents_v3

# Embeddings
EMBEDDING_MODEL=Alibaba-NLP/gte-multilingual-base
EMBEDDING_BATCH_SIZE=64
EMBEDDING_USE_PREFIXES=true

# Reranking
RERANKER_MODEL=jinaai/jina-reranker-v2-base-multilingual
RERANKER_BATCH_SIZE=16

# LLM Provider (huggingface, ollama, vllm, api)
LLM_PROVIDER=huggingface
LLM_MODEL=Qwen/Qwen2.5-7B-Instruct
LLM_LOAD_IN_4BIT=true
LLM_MAX_NEW_TOKENS=2000
LLM_TEMPERATURE=0.2

# Alternative: Ollama
# LLM_PROVIDER=ollama
# OLLAMA_HOST=http://localhost:11434
# LLM_MODEL_NAME_OLLAMA=qwen3:4b

# Alternative: API (LM Studio, OpenAI-compatible)
# LLM_PROVIDER=api
# LLM_API_BASE_URL=http://localhost:1234/v1
# LLM_API_MODEL_NAME=local-model
# LLM_API_KEY=your-key

# Query Rewriting ğŸ†•
REWRITE_ENABLED=true
REWRITE_TEMPERATURE=0.2
REWRITE_MAX_TOKENS=4096
REWRITE_VERIFY_BEFORE_RETRIEVAL=false

# Multihop Configuration ğŸ†•
MULTIHOP_FUSION_STRATEGY=max           # max, mean, weighted_mean
MULTIHOP_GLOBAL_RANKER_WEIGHT=0.6     # 0.0-1.0
MULTIHOP_TOP_K_PER_SUBQUERY=20
MULTIHOP_FINAL_TOP_K=10

# Retrieval Pipeline
TOP_K_RETRIEVE=100     # Initial retrieval
RERANK_TOP_M=80        # Candidates for reranking
CONTEXT_TOP_N=8        # Final chunks to LLM

# Chunking
CHUNKER_STRATEGY=semantic
CHUNK_SIZE=512
CHUNK_OVERLAP=96
```

---

## ğŸ“– Usage

### Command-Line Interface

```bash
# Ingestion pipelines
python -m src.ragx.ingestion.pipelines --help

# Available commands:
python -m src.ragx.ingestion.pipelines download --language pl
python -m src.ragx.ingestion.pipelines ingest <source> --max-articles 10000
python -m src.ragx.ingestion.pipelines status
python -m src.ragx.ingestion.pipelines search "query text"
```

### Makefile Commands

```bash
# Setup
make install              # Install dependencies
make setup-qdrant         # Start Qdrant container

# Wikipedia Pipeline
make download-wiki        # Download PL Wikipedia dump
make extract-wiki         # Extract articles to JSON
make ingest-test          # Test ingestion (1k articles)
make ingest-full          # Full ingestion (200k articles)

# Progress Tracking
make ingest-resume        # Resume from last processed file
make ingest-from FILE=wiki_05  # Start from specific file
make status-detailed      # Show file-by-file history

# API Server
make api                  # Start FastAPI server
make api-dev              # Start with auto-reload

# Search & Status
make search QUERY="..."   # Search for query
make status               # Check system status

# Maintenance
make clean                # Clean cache files
make clean-data           # Clean data files
make clean-all            # Nuclear clean
```

---

## ğŸŒ API Documentation

### Endpoints Overview

```
GET  /api                      # API information
GET  /info/health              # Health check with model status

POST /ask/baseline             # Simple RAG pipeline
POST /ask/enhanced             # Enhanced pipeline with query rewriting

POST /llm/generate             # Direct LLM access (no RAG)

POST /search/search            # Vector search only
POST /search/rerank            # Search + reranking

POST /analysis/linguistic      # Linguistic analysis
POST /analysis/multihop        # Multihop search with detailed metadata
```

### Example Requests

#### 1. Baseline Pipeline (Simple RAG)

```bash
curl -X POST "http://localhost:8000/ask/baseline" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Co to jest sztuczna inteligencja?",
    "top_k": 5
  }'
```

**Response:**
```json
{
  "answer": "Sztuczna inteligencja (SI) to dziaÅ‚ informatyki zajmujÄ…cy siÄ™...",
  "sources": [
    {
      "id": "doc123",
      "doc_title": "Sztuczna inteligencja",
      "text": "...",
      "retrieval_score": 0.85,
      "rerank_score": null
    }
  ],
  "metadata": {
    "pipeline": "baseline",
    "retrieval_time_ms": 12.5,
    "llm_time_ms": 850.2,
    "total_time_ms": 862.7,
    "num_sources": 5
  }
}
```

#### 2. Enhanced Pipeline (Query Rewriting + Multihop)

```bash
curl -X POST "http://localhost:8000/ask/enhanced" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ziemniaki vs pomidory, co ma wiÄ™cej bÅ‚onnika?",
    "top_k": 8
  }'
```

**Response:**
```json
{
  "answer": "Ziemniaki zawierajÄ… okoÅ‚o 2.2g bÅ‚onnika na 100g [1][2], podczas gdy pomidory okoÅ‚o 1.2g [3][4]...",
  "sources": [
    {
      "id": "doc456",
      "doc_title": "Ziemniaki",
      "text": "...",
      "local_rerank_score": 0.92,
      "fused_score": 0.88,
      "global_rerank_score": 0.85,
      "final_score": 0.87,
      "fusion_metadata": {
        "source_subqueries": ["Ile bÅ‚onnika majÄ… ziemniaki?"],
        "num_occurrences": 1
      }
    }
  ],
  "metadata": {
    "pipeline": "enhanced",
    "is_multihop": true,
    "sub_queries": [
      "Ile bÅ‚onnika majÄ… ziemniaki?",
      "Ile bÅ‚onnika majÄ… pomidory?"
    ],
    "query_type": "comparison",
    "reasoning": "comparison by fiber",
    "rewrite_time_ms": 450.2,
    "retrieval_time_ms": 25.8,
    "rerank_time_ms": 180.5,
    "llm_time_ms": 920.1,
    "total_time_ms": 1576.6,
    "num_candidates": 200,
    "num_sources": 8
  }
}
```

#### 3. Linguistic Analysis

```bash
curl -X POST "http://localhost:8000/analysis/linguistic" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Co Å‚Ä…czy mitologiÄ™ sÅ‚owiaÅ„skÄ… i nordyckÄ…?"
  }'
```

**Response:**
```json
{
  "query": "Co Å‚Ä…czy mitologiÄ™ sÅ‚owiaÅ„skÄ… i nordyckÄ…?",
  "pos_sequence": ["PRON", "VERB", "NOUN", "ADJ", "CCONJ", "ADJ"],
  "dep_tree": [
    {"dependency": "nsubj", "head": "Å‚Ä…czy", "child": "Co"},
    {"dependency": "ROOT", "head": "Å‚Ä…czy", "child": "Å‚Ä…czy"}
  ],
  "entities": [
    {"text": "mitologiÄ™ sÅ‚owiaÅ„skÄ…", "label": "MISC"},
    {"text": "nordyckÄ…", "label": "MISC"}
  ],
  "num_tokens": 7,
  "num_clauses": 1,
  "syntax_depth": 3,
  "has_relative_clauses": false,
  "has_conjunctions": true,
  "analysis_text": "..."
}
```

#### 4. Multihop Search with Options

```bash
curl -X POST "http://localhost:8000/analysis/multihop" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "krÃ³l Mieszko I czy BolesÅ‚aw Chrobry miaÅ‚ wiÄ™kszy wpÅ‚yw?",
    "top_k": 10,
    "use_reranker": true,
    "include_linguistic_analysis": true
  }'
```

**Features:**
- `use_reranker`: Enable/disable three-stage reranking (default: true)
- `include_linguistic_analysis`: Add linguistic features to response (default: false)
- Automatic query decomposition
- Query-type-specific fusion strategies

#### 5. Direct LLM Access (No RAG)

```bash
curl -X POST "http://localhost:8000/llm/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "WyjaÅ›nij czym jest gradient descent",
    "temperature": 0.7,
    "max_tokens": 500,
    "chain_of_thought_enabled": true
  }'
```

---

## ğŸ“ Project Structure

```
ragx/
â”œâ”€â”€ src/ragx/
â”‚   â”œâ”€â”€ api/                        # FastAPI server
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py            # /ask endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py        # /analysis endpoints ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py          # /search endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py             # /llm endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py          # /info endpoints
â”‚   â”‚   â”œâ”€â”€ schemas/               # Pydantic models
â”‚   â”‚   â””â”€â”€ dependencies.py        # DI container
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                 # Data ingestion pipeline
â”‚   â”‚   â”œâ”€â”€ chunkers/
â”‚   â”‚   â”‚   â””â”€â”€ chunker.py         # Semantic & token-based chunking
â”‚   â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”‚   â”œâ”€â”€ ingestion_pipeline.py
â”‚   â”‚   â”‚   â””â”€â”€ ingestion_progress.py  # Progress tracking
â”‚   â”‚   â””â”€â”€ extractions/
â”‚   â”‚       â””â”€â”€ wiki_extractor.py  # Wikipedia extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                 # Retrieval & reranking
â”‚   â”‚   â”œâ”€â”€ embedder/
â”‚   â”‚   â”‚   â””â”€â”€ embedder.py        # Bi-encoder embeddings
â”‚   â”‚   â”œâ”€â”€ rerankers/
â”‚   â”‚   â”‚   â””â”€â”€ reranker.py        # Cross-encoder reranking
â”‚   â”‚   â”œâ”€â”€ analyzers/ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ linguistic_analyzer.py   # spaCy analysis
â”‚   â”‚   â”‚   â””â”€â”€ linguistic_features.py   # Feature dataclass
â”‚   â”‚   â”œâ”€â”€ rewriters/ ğŸ†•
â”‚   â”‚   â”‚   â”œâ”€â”€ adaptive_rewriter.py     # Query rewriting
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rewriter_prompts.yaml
â”‚   â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚   â”‚       â””â”€â”€ parse.py             # JSON validation
â”‚   â”‚   â”œâ”€â”€ constants/ ğŸ†•
â”‚   â”‚   â”‚   â””â”€â”€ query_types.py           # Query type definitions
â”‚   â”‚   â””â”€â”€ vector_stores/
â”‚   â”‚       â””â”€â”€ qdrant_store.py    # Qdrant integration
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/ ğŸ†•              # RAG pipelines
â”‚   â”‚   â”œâ”€â”€ base.py                # Abstract base
â”‚   â”‚   â”œâ”€â”€ baseline.py            # Simple RAG
â”‚   â”‚   â”œâ”€â”€ enhanced.py            # Advanced RAG
â”‚   â”‚   â””â”€â”€ enhancers/
â”‚   â”‚       â”œâ”€â”€ reranker.py        # Standard reranking
â”‚   â”‚       â””â”€â”€ multihop_reranker.py  # Multihop reranking
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/                # LLM & prompting
â”‚   â”‚   â”œâ”€â”€ model.py               # LLM loading (HuggingFace)
â”‚   â”‚   â”œâ”€â”€ inference.py           # Multi-provider inference
â”‚   â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”‚   â”œâ”€â”€ ollama_provider.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vllm_provider.py
â”‚   â”‚   â”‚   â””â”€â”€ api_provider.py
â”‚   â”‚   â””â”€â”€ prompts/
â”‚   â”‚       â”œâ”€â”€ builder.py         # Prompt templates
â”‚   â”‚       â””â”€â”€ templates/
â”‚   â”‚           â”œâ”€â”€ basic.yaml
â”‚   â”‚           â”œâ”€â”€ enhanced.yaml
â”‚   â”‚           â””â”€â”€ multihop.yaml  ğŸ†•
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ settings.py            # Configuration management
â”‚       â”œâ”€â”€ logging_config.py      # Structured logging
â”‚       â””â”€â”€ model_registry.py      # Model caching
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ models.yaml                # Model configurations
â”‚   â””â”€â”€ app.yaml                   # App settings
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Raw Wikipedia dumps
â”‚   â”œâ”€â”€ processed/                 # Extracted articles
â”‚   â””â”€â”€ .ingestion_progress.json  # Progress tracking
â”‚
â”œâ”€â”€ tests/                         # Unit & integration tests
â”œâ”€â”€ docs/                          # Additional documentation
â”œâ”€â”€ docker-compose.yml             # Qdrant service
â”œâ”€â”€ Makefile                       # Development commands
â”œâ”€â”€ pyproject.toml                 # Project dependencies
â””â”€â”€ README.md                      # This file
```

---

## ğŸ“Š Performance

### Benchmarks (Single GPU - RTX 4070)

| Operation | Speed | Notes |
|-----------|-------|-------|
| Embedding (batch=64) | ~1200 docs/s | GTE-multilingual-base |
| Reranking (batch=16) | ~80 pairs/s | Jina-reranker-v2 |
| Chunking (semantic) | ~50 docs/s | LlamaIndex splitter |
| LLM Generation (HF) | ~25 tokens/s | Qwen2.5-7B (4-bit) |
| LLM Generation (Ollama) | ~40 tokens/s | Qwen3:4b |
| Vector Search | <10ms | Qdrant HNSW (100k docs) |
| Query Rewriting | ~450ms | LLM-based decomposition |
| Linguistic Analysis | ~50ms | spaCy Polish model |

### Multihop Query Performance

| Query Type | Stages | Time | Notes |
|------------|--------|------|-------|
| Simple | Standard | ~900ms | Single retrieval + rerank |
| Comparison (2 entities) | Multihop | ~1600ms | 2 sub-queries + 3-stage rerank |
| Similarity (2 entities) | Multihop | ~1700ms | 2 sub-queries + fusion |
| Chaining (3 hops) | Multihop | ~2200ms | 3 sub-queries + global rerank |
| Aggregation | Multihop | ~2500ms | Multiple retrievals + fusion |

**Optimization Tips:**
- Use Ollama/vLLM for faster LLM inference
- Disable query rewriting for simple lookups
- Adjust `MULTIHOP_TOP_K_PER_SUBQUERY` for speed/quality tradeoff
- Use `use_reranker=false` in multihop endpoint for faster fusion-only

### Scalability

| Corpus Size | Index Time | Search Time | Memory |
|-------------|------------|-------------|--------|
| 10k docs    | ~5 min     | <10ms | 2GB    |
| 200k docs   | ~3 hours   | <15ms | 8GB    |
| 1M docs     | ~8 hours   | <30ms | 40GB   |

---

## ğŸ¯ Roadmap

### âœ… Completed (v0.2)
- [x] Basic RAG pipeline (retrieval â†’ LLM)
- [x] Cross-encoder reranking
- [x] Semantic chunking with LlamaIndex
- [x] Qdrant integration with HNSW
- [x] Progress tracking & resume
- [x] Multi-lingual support (PL/EN)
- [x] **Linguistic analysis with spaCy** ğŸ†•
- [x] **Adaptive query rewriting** ğŸ†•
- [x] **Multihop retrieval with three-stage reranking** ğŸ†•
- [x] **Query type detection (8 types)** ğŸ†•
- [x] **Multi-provider LLM support (HF/Ollama/vLLM/API)** ğŸ†•
- [x] **Advanced prompting with templates** ğŸ†•
- [x] **FastAPI REST server** ğŸ†•

### ğŸš§ In Progress (v0.3)
- [ ] Self-Verification (CoVe) implementation
- [ ] Web UI for search
- [ ] Batch evaluation framework
- [ ] Query expansion with embedding similarity
- [ ] RAG fusion techniques

### ğŸ”® Planned (v0.4+)
- [ ] Hybrid search (BM25 + vector)
- [ ] Multi-modal support (images, tables)
- [ ] Multi-hop reasoning with graphs
- [ ] Fine-tuning scripts (LoRA/QLoRA)
- [ ] Deployment guides (Docker, K8s)
- [ ] Streaming responses
- [ ] Query caching

### Development Setup

```bash
# Install dev dependencies
make dev

# Pre-commit hooks
pre-commit install

# Code formatting
make fmt

# Linting
make lint
```

### Code Style
- **Formatter:** `black` + `isort`
- **Linter:** `ruff`
- **Type checking:** `mypy`
- **Docstrings:** Google style

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### Papers & Methods
- **RAG:** [Retrieval-Augmented Generation (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)
- **Query Rewriting:** [Query Rewriting for Retrieval (Jagerman et al., 2023)](https://arxiv.org/abs/2305.14283)
- **Multihop QA:** [HotpotQA (Yang et al., 2018)](https://arxiv.org/abs/1809.09600)
- **Cross-Encoders:** [Sentence-BERT (Reimers & Gurevych, 2019)](https://arxiv.org/abs/1908.10084)
- **CoVe:** [Chain-of-Verification (Dhuliawala et al., 2023)](https://arxiv.org/abs/2309.11495)

### Libraries & Tools
- [Sentence-Transformers](https://www.sbert.net/) - Embedding & reranking
- [Qdrant](https://qdrant.tech/) - Vector database
- [LlamaIndex](https://www.llamaindex.ai/) - Semantic chunking
- [spaCy](https://spacy.io/) - Linguistic analysis
- [Transformers](https://huggingface.co/transformers/) - LLM inference
- [FastAPI](https://fastapi.tiangolo.com/) - REST API framework

### Models
- **GTE-multilingual** (Alibaba)
- **Jina-reranker-v2** (Jina AI)
- **Qwen3 / Qwen3** (Alibaba Cloud)


## ğŸ”¥ Quick Examples

### Example 1: Simple Lookup (Baseline)

```bash
$ curl -X POST "http://localhost:8000/ask/baseline" \
  -d '{"query": "Co to jest Warszawa?"}'

{
  "answer": "Warszawa to stolica Polski [1] i najwiÄ™ksze miasto kraju...",
  "metadata": {
    "pipeline": "baseline",
    "total_time_ms": 850.5
  }
}
```

### Example 2: Comparison Query (Enhanced with Multihop)

```bash
$ curl -X POST "http://localhost:8000/ask/enhanced" \
  -d '{"query": "ziemniaki vs pomidory bÅ‚onnik"}'

{
  "answer": "Ziemniaki zawierajÄ… 2.2g bÅ‚onnika na 100g [1][2], 
             pomidory 1.2g [3][4]. Ziemniaki majÄ… wiÄ™cej bÅ‚onnika.",
  "metadata": {
    "is_multihop": true,
    "sub_queries": [
      "Ile bÅ‚onnika majÄ… ziemniaki?",
      "Ile bÅ‚onnika majÄ… pomidory?"
    ],
    "query_type": "comparison",
    "total_time_ms": 1576.6
  }
}
```

### Example 3: Verification Query

```bash
$ curl -X POST "http://localhost:8000/ask/enhanced" \
  -d '{"query": "Polska to najwiÄ™kszy kraj europejski?"}'

{
  "answer": "Nie, Polska nie jest najwiÄ™kszym krajem w Europie. 
             Polska ma powierzchniÄ™ 312,696 kmÂ² [1], podczas gdy 
             najwiÄ™kszym krajem Europy jest Rosja... [2]",
  "metadata": {
    "is_multihop": true,
    "query_type": "verification",
    "reasoning": "verification of superlative claim"
  }
}
```

### Example 4: Linguistic Analysis Only

```python
from src.ragx.retrieval.analyzers.linguistic_analyzer import LinguisticAnalyzer

analyzer = LinguisticAnalyzer()
features = analyzer.analyze("Co Å‚Ä…czy mitologiÄ™ sÅ‚owiaÅ„skÄ… i nordyckÄ…?")

print(f"Tokens: {features.num_tokens}")
print(f"Clauses: {features.num_clauses}")
print(f"Entities: {features.entities}")
print(f"Has conjunctions: {features.has_conjunctions}")
```

### Example 5: Python API

```python
from src.ragx.pipelines.enhanced import EnhancedPipeline

# Initialize pipeline
pipeline = EnhancedPipeline()

# Ask a complex question
result = pipeline.answer(
    query="KrÃ³l Mieszko I czy BolesÅ‚aw Chrobry miaÅ‚ wiÄ™kszy wpÅ‚yw?",
    top_k=10
)

print(f"Answer: {result['answer']}")
print(f"Is multihop: {result['metadata']['is_multihop']}")
print(f"Sub-queries: {result['metadata']['sub_queries']}")
print(f"Query type: {result['metadata'].get('query_type')}")
print(f"Sources: {len(result['sources'])}")
```

---

**Happy RAG-ing! ğŸš€**
